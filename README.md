## CSDS 395 Case Western Reserve University - Sentify Progress Report 2
* Margaret Butterfield
* Yash Goswami
* Christine Pan
* Stamatis Papadopoulos
* Alexander Rambasek
* Andrei Tiu
* William Turner

# Background 
## Introduction
As more vaccines are administered and pandemic restrictions are beginning to loosen, people across the country are once again looking for the best dining options. Most people who consult popular restaurant review sites like Yelp and FourSquare are usually only there to look at the number of stars out of five and skim the first couple reviews towards the top of the list. Nobody has time to meticulously page through hundreds of reviews to make an informed decision about where to dine out. As a result, the vast mountain of data presented to the user is, in its current state, useless. This is where our project, Sentify, comes in.

Sentify (a combination of “sentiment” and “identify”) is a web application that aims to enhance the dining experience by extracting valuable information from user reviews and presenting it in an intuitive way. Specifically, Sentify leverages current advancements in natural language processing to extract key aspects and sentiments from user reviews. This is done by using the Bidirectional Encoder Representations from Transformers (BERT) language model developed by Google. Once the key aspects and sentiments are extracted, Sentify can use this information to score the individual qualities of the restaurant (e.g. food, atmosphere, service). Users are able to search for restaurants nearby and view these statistics generated by Sentify. Furthermore, users can explicitly filter for restaurants based on aspects they care about.

## Machine Learning Model
The machine learning model that is used for this project is Google’s BERT, as mentioned in the introduction. BERT is a state-of-the-art model in the subfield of natural language processing, and it has been successfully applied to tasks such as natural language generation and next-sentence prediction. BERT is a transformer-based model, which means that it utilizes what is known as attention. Attention mechanisms help the model learn which parts of the data are most useful for the task and subsequently construct feature representations that are more suited towards the learning task. BERT was trained on the entire English Wikipedia corpus.

In this project, we adapt BERT for a task known as Aspect-Based Sentiment Analysis (ABSA). BERT is chosen as the model framework for this project because its bidirectional structure allows it to use contextual information from surrounding words, something that is lost in a bag-of-words approach. This NLP task is an amalgamation of Named Entity Recognition (locate and classify named entities in text) and Sentiment Analysis (identify the sentiment/polarity associated with text). We have decided to combine both tasks into one to train BERT with. This task can be formalized as a multi-label classification task, with class labels {positive, negative, neutral, conflicted, none}. A conflicted sentiment is when both positive and negative sentiments are expressed in relation to the same topic. “Neutral” means that there was no polarity concerning the topic, and “none” means that the topic did not appear. The input to the model is the tokenized review text, and the model has an attention module for every aspect that is cared about (e.g., food, price, ambience, customer service). For instance, the input “It took a while for our food to arrive, but it was well worth the wait” is expected to result in {“food:” positive, “price:” none, “ambience:” none, “customer service:” negative}.

A number of papers applying BERT to ABSA exist, and this is where we are sourcing a lot of information about the specifics of the model. As shown in [1], BERT has been very successful when applied to this type of task, and this paper provides the main inspiration for our implementation of the model. A number of variations on BERT exist for this task that we hope to also incorporate. For instance, [2] describes a method for boosting the performance of BERT by post-training the model on domain-specific corpuses. Also, [3] describes an adversarial training methodology, attempting to fool BERT by presenting it with deceptive input.

The dataset that we are using to train BERT is adapted from the International Workshop on Semantic Evaluation (Sem-Eval 2014). Examples consist of short snippets of restaurant review text, accompanied by tags corresponding to aspect content and corresponding semantics. Since the dataset is unbalanced with respect to class labelings, we may amend the dataset with some of our own hand-labeled examples.

## API Calls Using AWS Lambda
Using the trained BERT model, the next part of the project is analyzing actual restaurant reviews. We have chosen to limit our scope to the Cleveland area to offer a dense array of options for our users. Using the Scale SERP API we are going to make 125 API calls a day (number of calls is limited by the API itself) to receive restaurant names and their reviews. Each of these reviews will then go through sentiment analysis to determine the positivity of our selected attributes. The information along with the original review will then be stored in a database. To run this processing and data collection, we are using Amazon Web Services Lambda tool, which is a serverless compute tool that lets users run code without provisioning resources. Lambda has the ability to make hundreds of API calls in seconds and then has a direct pipeline to our database tool. 

## AWS Elastic Beanstalk Website
The next progression in the website is getting HTML widget functionality established so that there is a usable UI for people to interact with. Currently we are leveraging Bootstrap to lay out our website in a grid like structure that can adjust with browser size. Currently we have 2 widgets on the main page. The search button and the random button. The search button is to allow users to search for a specific restaurant and the random button is to return a random restaurant’s sentiment analysis. 

# Addressing Feedback from Progress Report 1
It was mentioned that more explanation was required for how the SemEval 2014 dataset would be transferred from 3 polarity labels to 5. In actuality, no transferral is required; the SemEval 2014 dataset contains 5 aspects (food, service, price, ambience, and anecdotes) and 4 polarity labels (positive, negative, neutral, and conflict). Our project is concerned with what SemEval calls their “subtask 4:” “Given a set of pre-identified aspect categories (e.g., {food, price}), determine the polarity (positive, negative, neutral or conflict) of each aspect category.” More can be found here: https://alt.qcri.org/semeval2014/task4/.

Reviews that we obtain from Lambda and ScaleSerp will not be labeled. This is because the data contained in SemEval and the data from ScaleSerp are restaurant reviews that are sourced from similar outlets such as Yelp and FourSquare. The SemEval data is split into training, validation, and testing sets. The performance of the model on the testing sets is assumed to be highly correlated with performance on novel data used in the web application. For the reasons discussed above, it is the opinion of the authors that this assumption is reasonable.


# Progress
Per our last report, our project should have reached a couple of milestones. We had set two personal goals, both of which are to be completed by the time this report is written. 

The first deadline we gave ourselves was to have the frontend functionality completed by October 25th. Our website has widgets for specific and random restaurant querying. It also has a description of what the site does and what parts of reviews we give results for.

The second milestone to reach was to have all sentiment analysis functions finished by October 30th. In terms of the model creation and training, this has been accomplished. A confusion matrix of one of the better-performing models is visible below. Although the model could theoretically be deployed as is, we do want to continue experimenting with different parameters to increase performance. For instance, the model is better at recognizing positive sentiments than negative ones, and has some difficulty identifying mixed/conflicted sentiments. Difficulty with conflicted sentiments is to be expected, and if time permits, additional data could be labeled to alleviate class imbalance and smooth model performance. However, the primary focus of the project from here on out will instead be directed towards the creation of the web application and the model integration. More model improvements will be treated as a secondary task.



# Future Completion Plan

| Event                                     | Date          |
| ------------------------------------------| --------------|
| Progress Report 2                         | November 1st  |
| Working Prototype with Sentiment Analysis | November 10th |
| Frontend Aesthetics Completed             | November 15th |
| Testing and Bug Fixing                    | November 20th |
| Details and Modeling of Performance       | November 27th |
| Final Project & Final Report              | December 1st  |
| Final Project Oral Presentation           | December 10th |

# Updated Management Plan

# Completed Work
* Website is deployed using Elastic Beanstalk with setup HTML in place. Is load-balanced and scalable.
* Lambda is set up and connected to DynamoDB. Has code set up to make API calls to get restaurant reviews and then stores data within DynamoDB.
* DynamoDB noSQL tables are set up and have testing objects inside them. Attributes have been defined to hold multiple reviews, restaurant name, and restaurant ID.
* S3 bucket is set up. Is currently storing past website versions and the BERT model.
* BERT model is trained and functional, although we will continue to experiment with improvements


# Member Contributions

Margaret:
* Worked on HTML widgets for website
* Worked on overall website design 
* Connected Lambda and DynamoDB
* Tested writing objects to DynamoDB via Lambda 
* Wrote sections of Project Report 2

Yash:
* Working more on the BERT model and training to see better fits.

Christine:
* Set up Project Report 2
* Filled in sections of Project Report 2
* Assisted in Bootstrap/HTML implementation
* Worked on website design and UI

Stamatis:
* Continued model training and tuning hyperparameters

Alexander:
* Writing code for model API interface
* Deploying model to Amazon SageMaker
* Wrote sections of PR2, primarily “Addressing Feedback”

Andrei:
* World on model training
* Working more on the BERT model

William:
* Configured review search API calls to run in lambda
* Began work on a dynamodb database to store results to avoid redundant search API calls
* Helped maintain website functionality 


